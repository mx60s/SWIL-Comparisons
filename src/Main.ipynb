{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dece910",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd / content/gdrive/MyDrive/cs394n_project/CS394N\n",
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f35190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/cs394n_project/CS394N/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1a94de-ed66-4b48-a600-9634eea3984f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.nets import *\n",
    "from utils.model_tools import *\n",
    "from utils.feature_extractor import *\n",
    "from utils.dataset_tools import *\n",
    "from utils.cosine_similarity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e794b7-8d3e-46d5-b629-4dd9ac25d12c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "LEARNING_RATE = 0.001 # Different for CIFAR100\n",
    "EXP_DECAY = 0.0001\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Files\n",
    "FNIST_model_no_boot_bag_file = \"./logs/fnist_no_boot_bag.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b952f719-964a-4e81-9d3e-67c079837e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1bf3162-271c-47b4-8dff-8f38b917338d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading general Fashion MNIST trainsets/testsets: https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "\n",
    "FMNIST_train_gen = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "FMNIST_trainloader_gen = torch.utils.data.DataLoader(FMNIST_train_gen, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "FMNIST_test_gen = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "FMNIST_testloader_gen = torch.utils.data.DataLoader(FMNIST_test_gen, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "FMNIST_classes = {'T-shirt/top', 'Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot'}\n",
    "\n",
    "\n",
    "# Paper leaves out 'Ankle boot' and 'Bag' class, which are indices 8 and 9 respectively\n",
    "# TODO: clean this up to use the nice subset code\n",
    "\n",
    "no_boot_bag_train_idx = np.where((np.array(FMNIST_train_gen.targets) != 8) & \n",
    "                        (np.array(FMNIST_train_gen.targets) != 9))[0]\n",
    "no_boot_bag_train_subset = torch.utils.data.Subset(FMNIST_train_gen, no_boot_bag_train_idx)\n",
    "no_boot_bag_train_dl = torch.utils.data.DataLoader(no_boot_bag_train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "no_boot_bag_test_idx = np.where((np.array(FMNIST_test_gen.targets) != 8) & \n",
    "                        (np.array(FMNIST_test_gen.targets) != 9))[0]\n",
    "no_boot_bag_test_subset = torch.utils.data.Subset(FMNIST_test_gen, no_boot_bag_test_idx)\n",
    "no_boot_bag_test_dl = torch.utils.data.DataLoader(no_boot_bag_test_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "boot_train_idx = np.where((np.array(FMNIST_train_gen.targets) == 9))[0]\n",
    "boot_train_subset = torch.utils.data.Subset(FMNIST_train_gen, boot_train_idx)\n",
    "boot_train_dl = torch.utils.data.DataLoader(boot_train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "no_bag_test_idx = np.where((np.array(FMNIST_test_gen.targets) != 8))[0]\n",
    "no_bag_test_subset = torch.utils.data.Subset(FMNIST_test_gen, no_bag_test_idx)\n",
    "no_bag_test_dl = torch.utils.data.DataLoader(no_bag_test_subset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test = torch.utils.data.ConcatDataset([boot_train_subset, no_bag_test_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5919ef2c-b222-42eb-b1bc-33d1def70032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "9000\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "print(len(boot_train_subset))\n",
    "print(len(no_bag_test_subset))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adceebe-3af9-4327-8b7f-46cdc7b72750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "linear_model = LinearFashionMNIST(8)\n",
    "FMNIST_optim = optim.Adam(linear_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "decay_rate = (EXP_DECAY/LEARNING_RATE)**(1/num_epochs)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=FMNIST_optim, gamma=decay_rate) \n",
    "# TODO: we need to use the scheduler for cnn too if we use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9ec36-9945-4ca5-b12e-b4764baa62bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training our base model with 8 classes\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(no_boot_bag_train_dl, linear_model, criterion, FMNIST_optim, 'cpu')\n",
    "    test_loss = test(no_boot_bag_test_dl, linear_model, criterion, 'cpu')\n",
    "    \n",
    "    print(\"Epoch\", epoch, \"train loss:\", train_loss, \"test loss:\", test_loss)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "print(\"Finished training, saving to\", FNIST_model_no_boot_bag_file)\n",
    "torch.save(linear_model.state_dict(), FNIST_model_no_boot_bag_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5b1c4-2ce5-41a6-b2f6-b852643f936d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting features from linear model trained on 8 classes\n",
    "\n",
    "linear_model = LinearFashionMNIST_alt(28*28, 8)\n",
    "\n",
    "fmnist_file = './weights/linear_fashionmnist_holdout_[8, 9].pt'\n",
    "\n",
    "linear_model.load_state_dict(torch.load(fmnist_file))\n",
    "linear_model.eval()\n",
    "\n",
    "last_layer = 'input_layer'\n",
    "print(\"Layer to be extracted:\", last_layer)\n",
    "\n",
    "fnist_feature_ext = FeatureExtractor(linear_model, [last_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1597724-3e34-41c1-8b25-7d8b832de316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fmnist_classes = list(range(8)) + [9]\n",
    "\n",
    "class_subsets, class_idxs, subset_size = generate_dls(FMNIST_train_gen, fmnist_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27c4bb-5e16-4c51-a907-9ec8a09a6d74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y, subset_size = extract_features(fnist_feature_ext, fmnist_classes, class_subsets, subset_size)\n",
    "avgs = get_lda_avgs(X, y, subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d31f86-67d5-4f6a-afda-3429185aa1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim_scores = get_similarity_vec(avgs)\n",
    "print(sim_scores)\n",
    "\n",
    "# this should show that our third-to-last and last values (for sandals and sneakers respectively) are\n",
    "# most similar to ankle boot (the closer to 0.5 the more similar)\n",
    "\n",
    "with open(r'./data/fmnist_sim_scores_boot.txt', 'w') as fp:\n",
    "    for s in sim_scores:\n",
    "        fp.write(\"%s\\n\" % s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870dbbd-1a56-425b-bd93-2cd26f15e19e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(r'./data/fmnist_sim_scores_boot.txt', 'r') as fp:\n",
    "    sim_scores = [float(i) for i in fp.readlines()]\n",
    "\n",
    "# Add new class to the linear model\n",
    "linear_model_new_class = add_output_nodes(fmnist_file)\n",
    "\n",
    "print(linear_model_new_class.state_dict())\n",
    "\n",
    "sim_sum = sum(sim_scores)\n",
    "\n",
    "sim_norms = [x/sim_sum for x in sim_scores]\n",
    "print(sim_norms)\n",
    "\n",
    "# they do this weird thing where the sample size for the boots class is set at 75 and everything not above a certain threshold is set to the same number of samples.\n",
    "# you can see in the paper (not appendix) this figure, we've mostly approximated it. They also specify N=350 total.\n",
    "boots_sample_size = 75\n",
    "sim_sample_sizes = [27 if x < 0.2 else int(x * boots_sample_size*3.52) for x in sim_norms] + [75]\n",
    "print(sim_sample_sizes)\n",
    "print(sum(sim_sample_sizes))\n",
    "\n",
    "fig = plt.figure(figsize = (8, 5))\n",
    "plt.bar([str(x) for x in fmnist_classes], sim_sample_sizes, color ='maroon', width = 0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10705ef6-8bf5-4e68-9a6d-5862b31f5475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataloader which contains the samples in the distribution as described above\n",
    "from random import sample\n",
    "\n",
    "sampled_idxs = []\n",
    "\n",
    "for i in range(len(fmnist_classes)):\n",
    "    idx_sample = sample(class_idxs[i].tolist(), sim_sample_sizes[i])\n",
    "    sampled_idxs += idx_sample\n",
    "\n",
    "swil_train_subset = torch.utils.data.Subset(FMNIST_train_gen, sampled_idxs)\n",
    "\n",
    "swil_train_dl = torch.utils.data.DataLoader(swil_train_subset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "# what to do for testing??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da94f6-015e-4445-a1ff-c1b24300d03f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, device, swap=False, swap_labels=[]) -> float:\n",
    "    '''\n",
    "        Model training loop. Performs a single epoch of model updates.\n",
    "        \n",
    "        * USAGE *\n",
    "        Within a training loop of range(num_epochs).\n",
    "\n",
    "        * PARAMETERS *\n",
    "        dataloader: A torch.utils.data.DataLoader object\n",
    "        model: A torch model which subclasses torch.nn.Module\n",
    "        loss_fn: A torch loss function, such as torch.nn.CrossEntropyLoss\n",
    "        optimizer: A torch.optim optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "\n",
    "        * RETURNS *\n",
    "        float: The model's average epoch loss \n",
    "    '''\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        if swap:\n",
    "            for i in range(len(y)):\n",
    "                if y[i] == swap_labels[0]:\n",
    "                    y[i] = swap_labels[1]\n",
    "                    \n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "\n",
    "        # Backpropagation\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append lists\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return train_loss/len(dataloader)\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn, device, swap=False, swap_labels=[], classes = 9) -> float:\n",
    "    '''\n",
    "        Model test loop. Performs a single epoch of model updates.\n",
    "\n",
    "        * USAGE *\n",
    "        Within a training loop of range(num_epochs) to perform epoch validation, or after training to perform testing.\n",
    "\n",
    "        * PARAMETERS *\n",
    "        dataloader: A torch.utils.data.DataLoader object\n",
    "        model: A torch model which subclasses torch.nn.Module\n",
    "        loss_fn: A torch loss function, such as torch.nn.CrossEntropyLoss\n",
    "        optimizer: A torch.optim optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "\n",
    "        * RETURNS *\n",
    "        float: The average test loss\n",
    "    '''\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    correct = [0] * classes\n",
    "    c = 0\n",
    "    #test_loss, correct = 0, 0\n",
    "    sizes = [0] * classes\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if swap:\n",
    "                for i in range(len(y)):\n",
    "                    if y[i] == swap_labels[0]:\n",
    "                        y[i] = swap_labels[1]\n",
    "                        \n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "            #if y.item() == 8:\n",
    "            #    print(\"pred:\", pred)\n",
    "            \n",
    "            # this is being appended incorrectly\n",
    "            # val_pred = np.append(val_pred, pred.detach().cpu().numpy()) \n",
    "            # targets = np.append(targets, y.detach().cpu().numpy())\n",
    "            \n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct[y.item()] += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            c += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            sizes[y.item()] += 1\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct = [x / s for x, s in zip(correct, sizes)]\n",
    "    c /= size\n",
    "    \n",
    "    \n",
    "    #print(torch.FloatTensor(val_pred).shape)\n",
    "    #print(torch.IntTensor(targets).shape)\n",
    "    \n",
    "    #recall = Recall(average='macro', num_classes=classes)\n",
    "    #recall_val = recall(torch.FloatTensor(val_pred), torch.IntTensor(targets))\n",
    "\n",
    "    print(\n",
    "        f\"Test Error: \\n Total accuracy:{(100*c):>0.1f}%, Accuracy 0: {(100*correct[0]):>0.1f}%, Accuracy 1: {(100*correct[1]):>0.1f}%, Accuracy 2: {(100*correct[2]):>0.1f}%, Accuracy 3: {(100*correct[3]):>0.1f}%, Accuracy 4: {(100*correct[4]):>0.1f}%, Accuracy 5: {(100*correct[5]):>0.1f}%, Accuracy 6: {(100*correct[6]):>0.1f}%, Accuracy 7: {(100*correct[7]):>0.1f}%, Accuracy 9: {(100*correct[8]):>0.1f}% \\n Avg loss: {test_loss:>8f} \\n\") #, Recall: {recall_val:>8f} \\n\")\n",
    "\n",
    "    return test_loss, correct, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26b6cd-edd7-4a85-8595-75eab566a304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# freeze first layer\n",
    "for param in linear_model_new_class.parameters():\n",
    "    param.requires_grad = False\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79c248-d230-4b33-8ac0-3869720e9a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = linear_model_new_class.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a8b7c-e4a7-491d-8b08-8d7638f01ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nets.LinearFashionMNIST_alt(28*28, 9)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "num_epochs = 6\n",
    "\n",
    "decay_rate = (EXP_DECAY/LEARNING_RATE)**(1/num_epochs)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate) \n",
    "\n",
    "model_file = './weights/linear_fashionmnist_holdout_[8]_frozen.pt'\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "#t = trange(num_epochs)\n",
    "t = range(num_epochs)\n",
    "accuracies_over_time0 = []\n",
    "accuracies_over_time1 = []\n",
    "accuracies_over_time2 = []\n",
    "accuracies_over_time3 = []\n",
    "accuracies_over_time4 = []\n",
    "accuracies_over_time5 = []\n",
    "accuracies_over_time6 = []\n",
    "accuracies_over_time7 = []\n",
    "accuracies_over_time9 = []\n",
    "total_acc_over_time = []\n",
    "\n",
    "for epoch in t:\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = train(swil_train_dl, model, loss_fn, optimizer, device, swap=True, swap_labels=[9,8])\n",
    "    test_loss, accuracies, acc = test(no_bag_test_dl, model, loss_fn, device, swap=True, swap_labels=[9,8])\n",
    "    accuracies_over_time0.append(accuracies[0])\n",
    "    accuracies_over_time1.append(accuracies[1])\n",
    "    accuracies_over_time2.append(accuracies[2])\n",
    "    accuracies_over_time3.append(accuracies[3])\n",
    "    accuracies_over_time4.append(accuracies[4])\n",
    "    accuracies_over_time5.append(accuracies[5])\n",
    "    accuracies_over_time6.append(accuracies[6])\n",
    "    accuracies_over_time7.append(accuracies[7])\n",
    "    accuracies_over_time9.append(accuracies[8])\n",
    "    print(accuracies_over_time9)\n",
    "    total_acc_over_time.append(acc)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "torch.save(model.state_dict(), model_file)\n",
    "print(\"Done!\")\n",
    "\n",
    "# need recall, accuracy (are we calculating that now?), cross-entropy loss (same?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43940f3a-a471-4658-a8da-38ee53305ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Total accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "# Convert the MSE loss values to RMSE\n",
    "\n",
    "#rmse_train_loss_log = np.sqrt(train_loss_log)\n",
    "#rmse_val_loss_log = np.sqrt(val_loss_log)\n",
    "\n",
    "#rmse_train_loss_log_ours = np.sqrt(train_loss_log_ours)\n",
    "#rmse_val_loss_log_ours = np.sqrt(val_loss_log_ours)\n",
    "\n",
    "plt.plot(total_acc_over_time, label='SWIL')\n",
    "plt.ylim([.6, 1])\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0becf-3bf1-4510-be0e-34eb347302c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# similar old classes\n",
    "\n",
    "old_sim_acc = []\n",
    "\n",
    "for a1, a2 in zip(accuracies_over_time5, accuracies_over_time7):\n",
    "    old_sim_acc.append((a1 + a2)/2)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Total accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.plot(old_sim_acc, label='SWIL')\n",
    "plt.ylim([.6, 1])\n",
    "plt.legend()\n",
    "plt.xlim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5cc3b-77d4-4101-932c-05221f484330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Different old classes\n",
    "\n",
    "old_sim_acc = []\n",
    "\n",
    "for a0, a1, a2, a3, a4, a6 in zip(accuracies_over_time0, accuracies_over_time1, accuracies_over_time2, accuracies_over_time3, accuracies_over_time4, accuracies_over_time6):\n",
    "    old_sim_acc.append((a0 + a1 + a2 + a3 + a4 + a6)/6)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Different old classes accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.plot(old_sim_acc, label='SWIL')\n",
    "plt.ylim([.6, 1])\n",
    "plt.legend()\n",
    "plt.xlim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b12e7d-89a6-4f30-b284-edd7aa226304",
   "metadata": {},
   "source": [
    "# CIFAR-10 CNN Adding New Class and Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
