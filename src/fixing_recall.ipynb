{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d8c425-dcdb-4e34-a38e-45b742f45a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.nets import *\n",
    "from utils.model_tools import *\n",
    "from utils.feature_extractor import *\n",
    "from utils.dataset_tools import *\n",
    "from utils.cosine_similarity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab15007-a1f9-4b32-9e8b-ee89098d15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EXP_DECAY = 0.0001\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "FMNIST_train_gen = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "FMNIST_trainloader_gen = torch.utils.data.DataLoader(FMNIST_train_gen, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "FMNIST_test_gen = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "FMNIST_testloader_gen = torch.utils.data.DataLoader(FMNIST_test_gen, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "no_boot_bag_train_idx = np.where((np.array(FMNIST_train_gen.targets) != 8) & \n",
    "                        (np.array(FMNIST_train_gen.targets) != 9))[0]\n",
    "no_boot_bag_train_subset = torch.utils.data.Subset(FMNIST_train_gen, no_boot_bag_train_idx)\n",
    "no_boot_bag_train_dl = torch.utils.data.DataLoader(no_boot_bag_train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "no_boot_bag_test_idx = np.where((np.array(FMNIST_test_gen.targets) != 8) & \n",
    "                        (np.array(FMNIST_test_gen.targets) != 9))[0]\n",
    "no_boot_bag_test_subset = torch.utils.data.Subset(FMNIST_test_gen, no_boot_bag_test_idx)\n",
    "no_boot_bag_test_dl = torch.utils.data.DataLoader(no_boot_bag_test_subset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f938929-4176-4db5-b476-5aec535e104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFashionMNIST_alt(nn.Module):\n",
    "  def __init__(self, input_size, num_classes: int):\n",
    "    super(LinearFashionMNIST_alt, self).__init__()\n",
    "\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.input_layer = nn.Linear(input_size, 128)\n",
    "    self.output_layer = nn.Linear(128, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.flatten(x)\n",
    "    return self.output_layer(self.input_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08823b85-c172-4e88-8c72-839203438829",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "linear_model = LinearFashionMNIST_alt(28*28, 8)\n",
    "FMNIST_optim = optim.Adam(linear_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "decay_rate = (EXP_DECAY/LEARNING_RATE)**(1/num_epochs)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=FMNIST_optim, gamma=decay_rate) \n",
    "# TODO: we need to use the scheduler for cnn too if we use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fbcdfab-50df-4a5c-9b96-c23689804ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.exceptions import ArchitectureError\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassRecall\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "165ce0cb-b98c-4aba-bd19-c2ffecf18df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, device, swap=False, swap_labels=[], classes = 9) -> float:\n",
    "    '''\n",
    "        Model test loop. Performs a single epoch of model updates.\n",
    "\n",
    "        * USAGE *\n",
    "        Within a training loop of range(num_epochs) to perform epoch validation, or after training to perform testing.\n",
    "\n",
    "        * PARAMETERS *\n",
    "        dataloader: A torch.utils.data.DataLoader object\n",
    "        model: A torch model which subclasses torch.nn.Module\n",
    "        loss_fn: A torch loss function, such as torch.nn.CrossEntropyLoss\n",
    "        optimizer: A torch.optim optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "\n",
    "        * RETURNS *\n",
    "        float: The average test loss\n",
    "    '''\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    y_pred_list, targets = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if swap:\n",
    "                for i in range(len(y)):\n",
    "                    if y[i] == swap_labels[0]:\n",
    "                        y[i] = swap_labels[1]\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            #preds.append(pred)\n",
    "            targets.append(y.numpy())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            _, y_pred_tags = torch.max(pred, dim=1)\n",
    "            y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "            \n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    #print(preds)\n",
    "    #print(targets)\n",
    "    \n",
    "    recall = MulticlassRecall(classes)\n",
    "    # torch.IntTensor(targets)\n",
    "    recall_val = recall(torch.FloatTensor(np.asarray(y_pred_list)), torch.IntTensor(np.asarray(targets)))\n",
    "    # should I be calling it on preds[0]?\n",
    "\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, Recall val: {recall_val:>8f} \\n\")\n",
    "\n",
    "    return test_loss, np.asarray(y_pred_list), np.asarray(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b6a694-747c-412e-9106-1d2ed9d8a60d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.016322  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.525051, Recall val: 0.719667 \n",
      "\n",
      "Epoch 0 train loss: 0.5428312357664108 test loss: 0.5250507748126984\n",
      "loss: 0.468086  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.491075, Recall val: 0.726222 \n",
      "\n",
      "Epoch 1 train loss: 0.4603755063811938 test loss: 0.49107469177246094\n",
      "loss: 0.425355  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.494720, Recall val: 0.723889 \n",
      "\n",
      "Epoch 2 train loss: 0.4429632571140925 test loss: 0.4947200751304626\n",
      "loss: 0.632280  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.471172, Recall val: 0.736556 \n",
      "\n",
      "Epoch 3 train loss: 0.4281905526717504 test loss: 0.47117196893692015\n",
      "loss: 0.453555  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.469281, Recall val: 0.737111 \n",
      "\n",
      "Epoch 4 train loss: 0.42134314419825875 test loss: 0.46928127670288083\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "y_preds = []\n",
    "y_tests = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(no_boot_bag_train_dl, linear_model, criterion, FMNIST_optim, 'cpu')\n",
    "    test_loss, y_pred_list, y_test = test(no_boot_bag_test_dl, linear_model, criterion, 'cpu')\n",
    "    y_preds.append(y_pred_list)\n",
    "    y_tests.append(y_test)\n",
    "    \n",
    "    print(\"Epoch\", epoch, \"train loss:\", train_loss, \"test loss:\", test_loss)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    #print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329ab204-cf4b-43c9-9bff-78959c9be84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2770000100135803, 0.2788749933242798, 0.2822500169277191, 0.2853749990463257, 0.28075000643730164]\n"
     ]
    }
   ],
   "source": [
    "target_classes = [1,2,6]\n",
    "num_classes = 8\n",
    "# I thought you would use len(target_classes), and instead I get this complaint: Detected more unique \n",
    "# values in `preds` than `num_classes`. Expected only 3 but found 6 in `preds`.\n",
    "\n",
    "recall_per_epoch = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    recall = MulticlassRecall(num_classes)\n",
    "    \n",
    "    y_per_epoch = np.asarray(y_tests[e]).flatten()\n",
    "    preds_per_epoch = np.asarray(y_preds[e]).flatten()\n",
    "    \n",
    "    condition = y_per_epoch == target_classes[0]\n",
    "    for i in range(1, len(target_classes)):\n",
    "        condition |= y_per_epoch == target_classes[i]\n",
    "    \n",
    "    target_y = np.extract(condition, y_per_epoch)\n",
    "    target_preds = np.extract(condition, preds_per_epoch)\n",
    "    \n",
    "    recall_val = recall(torch.IntTensor(target_preds), torch.IntTensor(target_y))\n",
    "    \n",
    "    recall_per_epoch.append(recall_val.item())\n",
    "        \n",
    "print(recall_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9287f797-7c9d-4d7f-a0a8-32153a7dfde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    }
   ],
   "source": [
    "target_classes = [1,2,3]\n",
    "recall_per_class = [[]] * len(target_classes)\n",
    "print(recall_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15ec3ab1-e287-468d-b127-ea9c769cc0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "[[0.889      0.81400001 0.83499998 0.82499999 0.85399997]\n",
      " [0.94700003 0.96200001 0.97100002 0.95099998 0.96200001]\n",
      " [0.80400002 0.61000001 0.68900001 0.73900002 0.74000001]\n",
      " [0.86500001 0.85799998 0.74000001 0.866      0.861     ]\n",
      " [0.63300002 0.727      0.764      0.74299997 0.76200002]\n",
      " [0.89600003 0.94999999 0.95899999 0.96200001 0.94      ]\n",
      " [0.465      0.65899998 0.59799999 0.59299999 0.54400003]\n",
      " [0.97799999 0.95599997 0.95899999 0.94999999 0.97100002]]\n"
     ]
    }
   ],
   "source": [
    "#target_classes = [1,2]\n",
    "nc = 8\n",
    "\n",
    "print(type(num_classes))\n",
    "\n",
    "recall_per_epoch = np.zeros([nc,num_epochs])\n",
    "recall = MulticlassRecall(nc, average=None)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    y_epoch = np.asarray(y_tests[i]).flatten()\n",
    "    yhat_epoch = np.asarray(y_preds[i]).flatten()\n",
    "    \n",
    "    y_epoch_ = []\n",
    "    yhat_epoch_ = []\n",
    "        \n",
    "    recall_val = recall(torch.IntTensor(yhat_epoch),torch.IntTensor(y_epoch))\n",
    "    recall_per_epoch[:,i] = recall_val\n",
    "    \n",
    "print(recall_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ed366f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fda75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
